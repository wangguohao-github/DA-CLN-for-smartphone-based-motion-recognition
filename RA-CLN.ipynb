{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Includes\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "# # physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# # tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# configuration = tf.compat.v1.ConfigProto()\n",
    "# configuration.gpu_options.allow_growth = True\n",
    "# session = tf.compat.v1.Session(config=configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"Accelerometer_x\",\n",
    "    \"Accelerometer_y\",\n",
    "    \"Accelerometer_z\",\n",
    "    \"Gyroscope_x\",\n",
    "    \"Gyroscope_y\",\n",
    "    \"Gyroscope_z\",\n",
    "    \"Gravity_x\",\n",
    "    \"Gravity_y\",\n",
    "    \"Gravity_z\",\n",
    "    \"Magnetometer_x\",\n",
    "    \"Magnetometer_y\",\n",
    "    \"Magnetometer_z\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(window_size,position):\n",
    "#     window_size=0.8\n",
    "    path=\"E:/polyu/代码/数据处理代码/new/data_for_dl_category by sheets_\"+position+\".xlsx_\" + str(window_size) + \".csv\"\n",
    "    data=pd.read_csv(path,header=0,index_col=None)\n",
    "    subject_names=[\"Rebar1\",\"Rebar2\",\"Rebar3\",\"Carpenter1\",\"Carpenter2\",\"Masonry1\",\"Masonry2\"]\n",
    "    data.loc[data[\"Label_2\"]==4,\"Label_2\"]=3\n",
    "    data.loc[data[\"Label_2\"]==5,\"Label_2\"]=4\n",
    "    data.loc[data[\"Label_2\"]==6,\"Label_2\"]=3\n",
    "    data.loc[data[\"Label_2\"]==7,\"Label_2\"]=3\n",
    "    data.loc[data[\"Label_2\"]==8,\"Label_2\"]=5\n",
    "    data.head(10)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generate(window_size,data,subject):\n",
    "    # data_temp=data.loc[[\"Rebar2\"],:]\n",
    "    # data_temp=data.loc[data[\"Subject\"]==\"Rebar1\" ,:]\n",
    "    window=int(window_size*50)\n",
    "    data_temp=data[data[\"Subject\"].isin(subject)]\n",
    "#     data_temp=data.loc[[\"Rebar2\"],:]\n",
    "#     data_temp=data.loc[data[\"Subject\"]==subject ,:]\n",
    "    data_subject=np.array(data_temp[data_temp[\"Label_2\"].isin([1,2,3,4,5])])\n",
    "    print(data_subject.shape)\n",
    "    data_subject=np.reshape(data_subject,(-1,window,15))\n",
    "    print(data_subject.shape)\n",
    "    X=data_subject[:,:,3:]\n",
    "    X=X.astype(\"float64\")\n",
    "    Y=data_subject[:,0,2]\n",
    "    Y=Y.astype(\"int\")\n",
    "    Group=data_subject[:,0,1]\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print(Group.shape)\n",
    "    return X,Y,Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据类别分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def distribution_check(Y):\n",
    "    Y_count=np.array(Y[:])\n",
    "    print(Y.shape)\n",
    "    print(Counter(Y_count))\n",
    "#     print(Counter(Group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整数编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def int_encoder(Y):\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y = label_encoder.fit_transform(Y)\n",
    "    print(Counter(Y))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义smote函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn as ib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "def smote(X_train,y_train,subject):\n",
    "    dim0=X_train.shape[0]\n",
    "    dim1=X_train.shape[1]\n",
    "    dim2=X_train.shape[2]\n",
    "    X_trian_for_smote=np.reshape(X_train,(dim0,dim1*dim2))\n",
    "    \n",
    "#     sme = SMOTETomek(random_state=42)\n",
    "#     X_train_smo, y_train = sme.fit_resample(X_trian_for_smote, y_train)\n",
    "#     rus = RandomUnderSampler(random_state=42,sampling_strategy='majority')\n",
    "#     X_res, y_res = rus.fit_resample(X_trian_for_smote, y_train)\n",
    "    if subject==\"Carpenter1\":\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={1:500,2:500})\n",
    "    elif subject==\"Carpenter2\":\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={2:500})\n",
    "#     elif subject==\"Masonry1\" or \"Masonry2\":\n",
    "#         smo = SMOTE(random_state=42,sampling_strategy={1:300})\n",
    "    elif subject==\"Rebar1\" or \"Rebar2\" or \"Rebar3\":\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={2:300,3:300})\n",
    "\n",
    "    X_train_smo, y_train = smo.fit_resample(X_trian_for_smote, y_train)\n",
    "#     cnn = CondensedNearestNeighbour(random_state=42,sampling_strategy='majority') \n",
    "#     X_res, y_train = cnn.fit_resample(X_trian_for_smote, y_train ) \n",
    "    X_train=np.reshape(X_train_smo,(X_train_smo.shape[0],dim1,dim2))\n",
    "    return X_train,y_train\n",
    "\n",
    "def smote_generalized(X_train,y_train,second):\n",
    "    dim0=X_train.shape[0]\n",
    "    dim1=X_train.shape[1]\n",
    "    dim2=X_train.shape[2]\n",
    "    X_trian_for_smote=np.reshape(X_train,(dim0,dim1*dim2))\n",
    "    \n",
    "    if second==0.6:\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={1:7000,2:7000,3:7000})\n",
    "    elif second==0.8:\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={1:5000,2:5000,3:5000}) \n",
    "    elif second==1:\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={1:4000,2:4000,3:4000})  \n",
    "\n",
    "    X_train_smo, y_train = smo.fit_resample(X_trian_for_smote, y_train)\n",
    "#     cnn = CondensedNearestNeighbour(random_state=42,sampling_strategy='majority') \n",
    "#     X_res, y_train = cnn.fit_resample(X_trian_for_smote, y_train ) \n",
    "    X_train=np.reshape(X_train_smo,(X_train_smo.shape[0],dim1,dim2))\n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import reshape\n",
    "from tensorflow.keras import backend as k\n",
    "class chan_attnlayer1(tf.keras.layers.Layer):\n",
    "    def __init__ (self,num_channels=12,ratio=0.05):\n",
    "        super(chan_attnlayer1,self).__init__()\n",
    "        self.num_channels=num_channels\n",
    "        self.ratio=ratio\n",
    "        self.avg_pool=tf.keras.layers.GlobalAveragePooling2D(data_format=\"channels_last\") #需要弄清楚channel是什么，在哪个维度上。\n",
    "        self.max_pool=tf.keras.layers.GlobalMaxPool2D(data_format=\"channels_last\")\n",
    "        self.fc1=tf.keras.layers.Dense(num_channels//ratio,activation=\"relu\")\n",
    "        self.fc2=tf.keras.layers.Dense(num_channels,activation= None)\n",
    "        self.softmax=tf.keras.layers.Activation('softmax')\n",
    "    def get_config(self):\n",
    "        config=super().get_config().copy()\n",
    "        config.update({\n",
    "            \"num_channels\":self.num_channels,\n",
    "            \"ratio\":self.ratio\n",
    "        })\n",
    "        return config\n",
    "    def call(self, inputs):\n",
    "        self.inputs1=tf.transpose(inputs, [0,1,3,2])\n",
    "#         print(self.inputs1.shape)\n",
    "        self.avg_out=self.fc2(self.fc1(self.avg_pool(self.inputs1)))\n",
    "#         print(self.avg_out.shape)\n",
    "        self.max_out=self.fc2(self.fc1(self.max_pool(self.inputs1)))\n",
    "#         print(self.max_out.shape)\n",
    "        self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "        self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "        self.max_out=tf.expand_dims(self.max_out,1)\n",
    "        self.max_out=tf.expand_dims(self.max_out,1)\n",
    "#         print(self.avg_out.shape)\n",
    "#         print(self.max_out.shape)\n",
    "        self.out=self.softmax(self.avg_out+self.max_out)\n",
    "#         self.out2=tf.multiply(self.inputs1,self.out)\n",
    "#         self.out1=tf.transpose(self.out,[0,1,3,2])\n",
    "#         print(self.out1.shape)\n",
    "        self.out2=tf.squeeze(tf.matmul(self.inputs1,tf.transpose(self.out,[0,1,3,2])),axis=-1)\n",
    "#         print(self.out2.shape)\n",
    "#         b,h,w,c=k.shape(self.out2)\n",
    "#         self.out3=tf.reshape(self.out,[-1,int(self.out2.shape[1]),int(self.out2.shape[2])*int(self.out2.shape[3])])\n",
    "#         self.out3=tf.reshape(self.out2,[-1,self.out2.shape[1],self.out2.shape[2]*self.out2.shape[3]])\n",
    "#         print(self.out2.shape)\n",
    "        return self.out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chan_attnlayer0(tf.keras.layers.Layer):\n",
    "    def __init__ (self,num_channels=12,ratio=0.05):\n",
    "        super(chan_attnlayer0,self).__init__()\n",
    "        self.num_channels=num_channels\n",
    "        self.ratio=ratio\n",
    "        self.avg_pool=tf.keras.layers.GlobalAveragePooling2D(data_format=\"channels_last\") #需要弄清楚channel是什么，在哪个维度上。\n",
    "        self.max_pool=tf.keras.layers.GlobalMaxPool2D(data_format=\"channels_last\")\n",
    "        self.fc1=tf.keras.layers.Dense(num_channels//ratio,activation=\"relu\")\n",
    "        self.fc2=tf.keras.layers.Dense(num_channels,activation= None)\n",
    "        self.sigmoid=tf.keras.layers.Activation('sigmoid')\n",
    "    def get_config(self):\n",
    "        config=super().get_config().copy()\n",
    "        config.update({\n",
    "            \"num_channels\":self.num_channels,\n",
    "            \"ratio\":self.ratio\n",
    "        })\n",
    "        return config\n",
    "    def call(self, inputs):\n",
    "#         print(inputs.shape)\n",
    "        self.inputs1=tf.transpose(inputs, [0,1,3,2])\n",
    "#         self.inputs1=tf.transpose(inputs, [0,1,2,3])\n",
    "#         print(self.inputs1.shape)\n",
    "        self.avg_out=self.fc2(self.fc1(self.avg_pool(self.inputs1)))\n",
    "#         print(self.avg_out.shape)\n",
    "        self.max_out=self.fc2(self.fc1(self.max_pool(self.inputs1)))\n",
    "#         print(self.max_out.shape)\n",
    "        self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "        self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "        self.max_out=tf.expand_dims(self.max_out,1)\n",
    "        self.max_out=tf.expand_dims(self.max_out,1)\n",
    "#         print(self.avg_out.shape)\n",
    "#         print(self.max_out.shape)\n",
    "        self.out=self.sigmoid(self.avg_out+self.max_out)\n",
    "#         print(self.out.shape)\n",
    "#         self.out2=tf.multiply(self.inputs1,self.out)\n",
    "#         self.out1=tf.transpose(self.out,[0,1,3,2])\n",
    "#         print(self.out1.shape)\n",
    "        self.out2=tf.transpose(tf.multiply(self.inputs1,self.out),[0,1,3,2])\n",
    "#         print(self.out2.shape)\n",
    "#         b,h,w,c=k.shape(self.out2)\n",
    "#         self.out3=tf.reshape(self.out,[-1,int(self.out2.shape[1]),int(self.out2.shape[2])*int(self.out2.shape[3])])\n",
    "#         self.out3=tf.reshape(self.out2,[-1,self.out2.shape[1],self.out2.shape[2]*self.out2.shape[3]])\n",
    "#         print(self.out2.shape)\n",
    "        return self.out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chan_attnlayer2(tf.keras.layers.Layer):\n",
    "    def __init__ (self,num_channels,ratio=16):\n",
    "        super(chan_attnlayer2,self).__init__()\n",
    "        self.num_channels=num_channels\n",
    "        self.ratio=ratio\n",
    "        self.avg_pool=tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_last\") #需要弄清楚channel是什么，在哪个维度上。\n",
    "        self.max_pool=tf.keras.layers.GlobalMaxPool1D(data_format=\"channels_last\")\n",
    "        self.fc1=tf.keras.layers.Dense(num_channels//ratio,activation=\"relu\")\n",
    "        self.fc2=tf.keras.layers.Dense(num_channels,activation= None)\n",
    "#         self.fc1=tf.keras.layers.Conv2D(num_channels,num_channels//ratio,3,activation=\"relu\")\n",
    "#         self.fc2=tf.keras.layers.Conv2D(num_channels//ratio,num_channels,3,activation= None)\n",
    "        self.sigmoid=tf.keras.layers.Activation('sigmoid')\n",
    "    def get_config(self):\n",
    "        config=super().get_config().copy()\n",
    "        config.update({\n",
    "            \"num_channels\":self.num_channels,\n",
    "            \"ratio\":self.ratio\n",
    "        })\n",
    "        return config\n",
    "    def call(self, inputs):\n",
    "#         print(inputs.shape)#[30,256]\n",
    "#         self.inputs1=tf.transpose(inputs, [0,1,3,2])\n",
    "#         self.inputs1=tf.transpose(inputs, [0,1,2,3])\n",
    "#         print(self.inputs1.shape)\n",
    "        self.avg_out=self.fc2(self.fc1(self.avg_pool(inputs)))\n",
    "#         print(self.avg_out.shape)\n",
    "        self.max_out=self.fc2(self.fc1(self.max_pool(inputs)))\n",
    "#         print(self.max_out.shape)\n",
    "        self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "#         self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "#         self.max_out=tf.expand_dims(self.max_out,1)\n",
    "        self.max_out=tf.expand_dims(self.max_out,1)\n",
    "#         print(self.avg_out.shape)\n",
    "#         print(self.max_out.shape)\n",
    "        self.out=self.sigmoid(self.avg_out+self.max_out)\n",
    "#         print(self.out.shape)\n",
    "#         self.out2=tf.multiply(self.inputs1,self.out)\n",
    "#         self.out1=tf.transpose(self.out,[0,1,3,2])\n",
    "#         print(self.out1.shape)\n",
    "        self.out2=tf.multiply(inputs,self.out)\n",
    "#         print(self.out2.shape)\n",
    "#         b,h,w,c=k.shape(self.out2)\n",
    "#         self.out3=tf.reshape(self.out,[-1,int(self.out2.shape[1]),int(self.out2.shape[2])*int(self.out2.shape[3])])\n",
    "#         self.out3=tf.reshape(self.out2,[-1,self.out2.shape[1],self.out2.shape[2]*self.out2.shape[3]])\n",
    "#         print(self.out2.shape)\n",
    "        return self.out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chan_attnlayer3(tf.keras.layers.Layer):\n",
    "    def __init__ (self,num_channels=256):\n",
    "        super(chan_attnlayer3,self).__init__()\n",
    "        self.num_channels=num_channels\n",
    "#         self.fc1=tf.keras.layers.Dense(num_channels//ratio,activation=\"relu\")\n",
    "#         self.fc2=tf.keras.layers.Dense(num_channels,activation= None)\n",
    "        self.fc1=tf.keras.layers.Conv2D(num_channels,3,dilation_rate=2,activation=\"relu\", padding='same')\n",
    "        self.fc2=tf.keras.layers.Conv2D(1,1,activation= None, padding='same')\n",
    "        self.sigmoid=tf.keras.layers.Activation('softmax')\n",
    "    def get_config(self):\n",
    "        config=super().get_config().copy()\n",
    "        config.update({\n",
    "            \"num_channels\":self.num_channels\n",
    "        })\n",
    "        return config\n",
    "    def call(self, inputs):\n",
    "#         print(inputs.shape)#[30,256]\n",
    "#         self.inputs1=tf.transpose(inputs, [0,1,3,2])\n",
    "#         self.inputs1=tf.transpose(inputs, [0,1,2,3])\n",
    "#         print(self.inputs1.shape)\n",
    "#         self.avg_out=self.fc2(self.fc1(self.avg_pool(inputs)))\n",
    "# #         print(self.avg_out.shape)\n",
    "#         self.max_out=self.fc2(self.fc1(self.max_pool(inputs)))\n",
    "#         print(self.max_out.shape)\n",
    "#         self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "# #         self.avg_out=tf.expand_dims(self.avg_out,1)\n",
    "# #         self.max_out=tf.expand_dims(self.max_out,1)\n",
    "#         self.max_out=tf.expand_dims(self.max_out,1)\n",
    "#         print(self.avg_out.shape)\n",
    "#         print(self.max_out.shape)\n",
    "#         self.out=self.sigmoid(self.avg_out+self.max_out)\n",
    "        self.inputs=tf.expand_dims(inputs,3)\n",
    "#         print(self.inputs.shape)\n",
    "        self.out=self.fc2(self.fc1(self.inputs))\n",
    "#         print(self.out.shape)\n",
    "#         print(self.avg_out.shape)\n",
    "#         self.max_out=self.fc2(self.fc1(self.max_pool(inputs)))\n",
    "#         print(self.out.shape)\n",
    "#         self.out2=tf.multiply(self.inputs1,self.out)\n",
    "#         self.out1=tf.transpose(self.out,[0,1,3,2])\n",
    "#         print(self.out1.shape)\n",
    "        self.out2=tf.multiply(inputs,tf.squeeze(self.out,axis=-1))\n",
    "#         print(self.out2.shape)\n",
    "#         b,h,w,c=k.shape(self.out2)\n",
    "#         self.out3=tf.reshape(self.out,[-1,int(self.out2.shape[1]),int(self.out2.shape[2])*int(self.out2.shape[3])])\n",
    "#         self.out3=tf.reshape(self.out2,[-1,self.out2.shape[1],self.out2.shape[2]*self.out2.shape[3]])\n",
    "#         print(self.out2.shape)\n",
    "        return self.out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporal_attention\n",
    "class attnlayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_outputs=128):\n",
    "        super(attnlayer, self).__init__()\n",
    "        self.n_outputs = n_outputs\n",
    "        self.fc1=tf.keras.layers.Dense(self.n_outputs,activation=\"tanh\")\n",
    "        self.fc2=tf.keras.layers.Dense(1,activation= \"softmax\")\n",
    "        \n",
    "#         self.activation1=activation1\n",
    "#         self.activation2=activation2\n",
    "#         self.tanh1=tf.keras.layers.Activation(activation1)\n",
    "#         self.softmax1=tf.keras.layers.Activation(activation2)\n",
    "#     def build(self):  # Create the state of the layer (weights)\n",
    "#         self.w = self.add_weight(name='w',\n",
    "#                                  shape=(64, self.n_outputs),\n",
    "#                                  initializer='uniform',\n",
    "#                                  trainable=True)\n",
    "#         self.bias = self.add_weight(name='bias',\n",
    "#                                  shape=(self.n_outputs,),\n",
    "#                                  initializer='zero',\n",
    "#                                  trainable=True)\n",
    "#         self.w1 = self.add_weight(name='w1',\n",
    "#                                  shape=(self.n_outputs, 1),\n",
    "#                                  initializer='uniform',\n",
    "#                                  trainable=True)\n",
    "#         self.bias1 = self.add_weight(name='bias1',\n",
    "#                                  shape=(1,),\n",
    "#                                  initializer='zero',\n",
    "#                                  trainable=True)\n",
    "    def get_config(self):\n",
    "        config=super().get_config().copy()\n",
    "        config.update({\n",
    "            \"n_outputs\":self.n_outputs\n",
    "        })\n",
    "        return config\n",
    "    def call(self, inputs):  # Defines the computation from inputs to outputs\n",
    "#         self.output1=self.tanh1(inputs @ self.w+self.bias)\n",
    "#         self.score=self.softmax1(self.output1@self.w1+self.bias1)\n",
    "        self.score=self.fc2(self.fc1(inputs))\n",
    "#         print(self.score)\n",
    "#         self.score1=self.score[1,:,:]\n",
    "        self.output2=tf.matmul(tf.transpose(inputs, [0,2,1]),self.score)\n",
    "#         print(self.output2.shape)\n",
    "        \n",
    "        return tf.squeeze(self.output2,axis=-1),self.score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LSTM\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "# # from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling1D, LSTM,AveragePooling1D\n",
    "# def construct_model(kernel_size,filters_num,pool,window_size_for_model,LSTM_num,class_num,conv_layer_num,learn_rate):\n",
    "#     model = Sequential()\n",
    "# #     for i in range(1,conv_layer_num+1):\n",
    "# #         if i==1:\n",
    "#     model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1),activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "            \n",
    "# #             model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1), activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "                \n",
    "# #             model.add(chan_attnlayer0())\n",
    "# # #             model.add(AveragePooling1D(pool_size=pool))\n",
    "# # #             model.add(MaxPooling1D(pool_size=pool))\n",
    "# #         else:\n",
    "# #             model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1), activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "            \n",
    "# #             model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1), activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "# # #             model.add(chan_attnlayer0())\n",
    "            \n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_2'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_2'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_3'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_3'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_4'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_5'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_4'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_5'),\n",
    "# #         Flatten(),\n",
    "# #     model.add(chan_attnlayer1())\n",
    "# #     model.add(tf.keras.Input(shape=(window_size_for_model,12)))\n",
    "# #     model.add(Dense(256, activation = 'relu'))\n",
    "#     model.add(tf.keras.layers.Reshape((30,-1)))\n",
    "#     model.add(LSTM(LSTM_num,return_sequences = True, name = 'lstm_1'))\n",
    "# #     model.add(LSTM(LSTM_num,input_shape=(window_size_for_model,filters_num),return_sequences = True, name = 'lstm_1'))\n",
    "#     model.add(Dropout(0.5, name = 'Dropout_1'))\n",
    "# #         BatchNormalization(name='batchnorm_layer_1'),\n",
    "#     model.add(LSTM(LSTM_num, return_sequences = False, name = 'lstm_2'))\n",
    "#     model.add(Dropout(0.5, name = 'Dropout_2'))\n",
    "# #         LSTM(LSTM_num, return_sequences = False, name = 'lstm_3'),\n",
    "# #         Dropout(0.4, name = 'Dropout_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_2'),\n",
    "# #     model.add(Dense(256, activation = 'sigmoid'))\n",
    "# #     model.add(Dense(256, activation = 'sigmoid'))\n",
    "# #     model.add(Flatten())\n",
    "# #     model.add(Dense(class_num, activation = 'softmax', name = 'output'))\n",
    "# #     model.add(attnlayer())\n",
    "#     #     Dense(128, activation= 'relu', name = 'dense_2'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_4'),\n",
    "#     #     Dense(64, activation = 'relu', name = 'dense_3'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_5'),\n",
    "# #         Dense(32, activation= 'relu', name = 'dense_1'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_3'),\n",
    "#     model.add(Dense(class_num, activation = 'softmax', name = 'output'))\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "# #     model.build(input_shape=(window_size_for_model,12))\n",
    "\n",
    "#     model.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#     # plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LSTM\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "# # from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling1D, LSTM,AveragePooling1D\n",
    "# def construct_model(kernel_size,filters_num,pool,window_size_for_model,LSTM_num,class_num,conv_layer_num,learn_rate):\n",
    "#     model = Sequential()\n",
    "# #     for i in range(1,conv_layer_num+1):\n",
    "# #         if i==1:\n",
    "# #             model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1),activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "            \n",
    "# #             model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1), activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "                \n",
    "# #             model.add(chan_attnlayer0())\n",
    "# # #             model.add(AveragePooling1D(pool_size=pool))\n",
    "# # #             model.add(MaxPooling1D(pool_size=pool))\n",
    "# #         else:\n",
    "# #             model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1), activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "            \n",
    "# #             model.add(Conv2D(filters=filters_num, kernel_size=kernel_size, strides=(1,1), activation='relu',padding = 'same',input_shape=(window_size_for_model,12,1)))\n",
    "# # #             model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "# #             model.add(BatchNormalization())\n",
    "# # #             model.add(chan_attnlayer0())\n",
    "            \n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_2'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_2'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_3'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_3'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_4'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_5'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_4'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_5'),\n",
    "# #         Flatten(),\n",
    "# #     model.add(chan_attnlayer1())\n",
    "#     model.add(tf.keras.Input(shape=(window_size_for_model,12)))\n",
    "#     model.add(Dense(256, activation = 'relu'))\n",
    "#     model.add(LSTM(LSTM_num,input_shape=(window_size_for_model,filters_num),return_sequences = True, name = 'lstm_1'))\n",
    "# #     model.add(LSTM(LSTM_num,input_shape=(window_size_for_model,filters_num),return_sequences = True, name = 'lstm_1'))\n",
    "#     model.add(Dropout(0.4, name = 'Dropout_1'))\n",
    "# #         BatchNormalization(name='batchnorm_layer_1'),\n",
    "#     model.add(LSTM(LSTM_num, return_sequences = False, name = 'lstm_2'))\n",
    "#     model.add(Dropout(0.4, name = 'Dropout_2'))\n",
    "# #         LSTM(LSTM_num, return_sequences = False, name = 'lstm_3'),\n",
    "# #         Dropout(0.4, name = 'Dropout_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_2'),\n",
    "# #     model.add(Dense(256, activation = 'sigmoid'))\n",
    "# #     model.add(Dense(256, activation = 'sigmoid'))\n",
    "# #     model.add(Flatten())\n",
    "# #     model.add(Dense(class_num, activation = 'softmax', name = 'output'))\n",
    "# #     model.add(attnlayer())\n",
    "#     #     Dense(128, activation= 'relu', name = 'dense_2'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_4'),\n",
    "#     #     Dense(64, activation = 'relu', name = 'dense_3'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_5'),\n",
    "# #         Dense(32, activation= 'relu', name = 'dense_1'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_3'),\n",
    "#     model.add(Dense(class_num, activation = 'softmax', name = 'output'))\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "# #     model.build(input_shape=(window_size_for_model,12))\n",
    "\n",
    "#     model.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#     # plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D卷积\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM,AveragePooling1D\n",
    "def construct_model(kernel_size,filters_num,pool,window_size_for_model,LSTM_num,class_num,conv_layer_num,learn_rate):\n",
    "    inputs=tf.keras.Input(shape=(window_size_for_model,12),name='img')\n",
    "    \n",
    "    h1=Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(inputs)\n",
    "    h1=BatchNormalization()(h1)\n",
    "    h1=tf.keras.layers.Activation(\"relu\")(h1)\n",
    "    h1=Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(h1)\n",
    "    h1=BatchNormalization()(h1)\n",
    "    \n",
    "#     h1=chan_attnlayer2(64)(h1)\n",
    "    h2=tf.keras.layers.Activation(\"relu\")(h1)\n",
    "    h2 = Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(h2)\n",
    "    h2=BatchNormalization()(h2)\n",
    "    h2=tf.keras.layers.Activation(\"relu\")(h2)\n",
    "    h2 = Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(h2)\n",
    "    h2=BatchNormalization()(h2)\n",
    "    block2_out = tf.keras.layers.add([h2, h1])\n",
    "\n",
    "#     h2=chan_attnlayer2()(h2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    h3=tf.keras.layers.Activation(\"relu\")(block2_out)\n",
    "    h3 = Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(h3)\n",
    "    h3=BatchNormalization()(h3)\n",
    "    h3=tf.keras.layers.Activation(\"relu\")(h3)\n",
    "    h3 = Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(h3)\n",
    "    h3=BatchNormalization()(h3)\n",
    "    block3_out = tf.keras.layers.add([h3, block2_out])\n",
    "    h3=tf.keras.layers.Activation(\"relu\")(block3_out)\n",
    "#     h3=chan_attnlayer2()(h3)\n",
    "#     block3_out = tf.keras.layers.add([h3, block2_out])\n",
    "    \n",
    "    h4=tf.keras.layers.Activation(\"relu\")(h3)\n",
    "    h4 = Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(h4)\n",
    "    h4=BatchNormalization()(h4)\n",
    "    h4=tf.keras.layers.Activation(\"relu\")(h4)\n",
    "    h4 = Conv1D(filters=filters_num, kernel_size=kernel_size, strides=1,padding = 'same')(h4)\n",
    "    h4=BatchNormalization()(h4)\n",
    "    block4_out = tf.keras.layers.add([h4, block3_out])\n",
    "    h4=tf.keras.layers.Activation(\"relu\")(block4_out)\n",
    "    \n",
    "    \n",
    "    h5=LSTM(LSTM_num,input_shape=(window_size_for_model,filters_num),return_sequences = True, name = 'lstm_1')(h4)\n",
    "    h5=tf.keras.layers.Dropout(0.5, name = 'Dropout_1')(h5)\n",
    "    \n",
    "    h6=LSTM(LSTM_num,input_shape=(window_size_for_model,filters_num),return_sequences = False, name = 'lstm_2')(h5)\n",
    "    h6=tf.keras.layers.Dropout(0.5, name = 'Dropout_2')(h6)\n",
    "#     h7=attnlayer()(h6)[0]\n",
    "#     score=attnlayer()(h6)[1]\n",
    "    \n",
    "    outputs=tf.keras.layers.Dense(class_num, activation = 'softmax', name = 'output')(h6)\n",
    "    \n",
    "    model=tf.keras.Model(inputs,outputs,name=\"residual_CLN\")\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    # plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from timeit import default_timer as timer\n",
    "def model_fit(model,X_train, y_train,X_validation,y_validation,batch_size,epochs):\n",
    "    earlystop_callback=EarlyStopping(monitor=\"val_loss\",patience=20)\n",
    "    modelcheckpoint=ModelCheckpoint(filepath=\"best_model.h5\",monitor=\"val_accuracy\",save_best_only=True)\n",
    "    start = timer()\n",
    "#     class_weight = {0 : 1., 1: 10., 2: 10., 3: 5., 4: 3.}\n",
    "\n",
    "    history = model.fit( X_train, y_train, epochs = epochs,validation_data = (X_validation,y_validation), batch_size=batch_size ,callbacks=[earlystop_callback,modelcheckpoint], verbose=1)\n",
    "#     history = model.fit( X_train, y_train, epochs = 200,validation_data = (X_validation,y_validation), batch_size=batch_size , verbose=1)\n",
    "    #     validation_data = (X_validation,y_validation)\n",
    "\n",
    "    end = timer()\n",
    "    print(\"\\n\")\n",
    "    print(\"Time: \",(end - start),\"secs = \",(end - start)/3600,\"hours\")\n",
    "\n",
    "\n",
    "    # ## EVALUATION AND THE PERFORMANCE METRICS\n",
    "\n",
    "    # In[62]:\n",
    "\n",
    "\n",
    "    # Plotting loss and accuracy graph\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    plt.plot(np.array(history.history['loss']), \"r--\", label=\"Train loss\")\n",
    "    plt.plot(np.array(history.history['accuracy']), \"g--\", label=\"Train accuracy\")\n",
    "\n",
    "    plt.plot(np.array(history.history['val_loss']), \"r-\", label=\"validation loss\")\n",
    "    plt.plot(np.array(history.history['val_accuracy']), \"g-\", label=\"validation accuracy\")\n",
    "\n",
    "    plt.title(\"Training session's progress over iterations\",fontsize=10)\n",
    "    plt.legend(loc='best', shadow=True,fontsize=10)\n",
    "    plt.ylabel('Training Progress (Loss or Accuracy values)',fontsize=10)\n",
    "    plt.xlabel('Training Epoch',fontsize=10)\n",
    "    plt.ylim(0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
    "def plot_confusion_matrix(cm, classes,class_num,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0,fontsize=25)\n",
    "    plt.yticks(tick_marks, classes,fontsize=25)\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(-.5, class_num-0.5)\n",
    "    ax.set_xlim(-.5, class_num-0.5)\n",
    "        \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",fontsize=20,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=20)\n",
    "    plt.xlabel('Predicted label',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型预测以及结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model,X_test,y_test,class_num,LABELS):\n",
    "#     results = dict()\n",
    "    test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "#     print(test_pred)\n",
    "    #输出分类结果\n",
    "#     print(y_test)\n",
    "#     print(test_pred)\n",
    "    classfication_report=metrics.classification_report(y_test,test_pred)\n",
    "    results=float(100*metrics.f1_score(y_test, test_pred, average=\"macro\"))\n",
    "#     results['classification_report'] = classification_report\n",
    "    print(classfication_report)\n",
    "    #输出混淆矩阵\n",
    "    cm = metrics.confusion_matrix(y_test, test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    #绘制归一化混淆矩阵\n",
    "    plt.figure(figsize=(class_num,class_num))\n",
    "    plt.grid(b=False)\n",
    "    plot_confusion_matrix(cm, LABELS,class_num, normalize=True, title='Normalized confusion matrix',cmap=plt.cm.Reds)\n",
    "    ax = plt.gca()\n",
    "    plt.show()\n",
    "    #绘制非归一化混淆矩阵\n",
    "    plt.figure(figsize=(class_num,class_num))\n",
    "    plot_confusion_matrix(cm, LABELS,class_num, normalize=False, title='confusion matrix',cmap=plt.cm.Reds)\n",
    "    ax = plt.gca()\n",
    "    plt.show()\n",
    "    return results,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf1 = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "# skf2 = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# logo = LeaveOneGroupOut()\n",
    "# # logo.get_n_splits(X, Y, group)\n",
    "\n",
    "# from sklearn.model_selection import LeavePGroupsOut\n",
    "# lpgo = LeavePGroupsOut(n_groups=2)\n",
    "\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# sgkf=StratifiedGroupKFold(n_splits=3,shuffle=True)\n",
    "\n",
    "# window_sizes=[0.6]\n",
    "# positions=[\"xd\"]\n",
    "# conv_layers_num=[2] #最优2\n",
    "# filters_nums=[256] #最优256\n",
    "# kernel_sizes=[3] #最优3\n",
    "# LSTM_nums=[64] #最优64\n",
    "\n",
    "# for position in positions:\n",
    "#     for window_size in window_sizes:\n",
    "#         for conv_layer_num in conv_layers_num:\n",
    "#             for filters_num in filters_nums:\n",
    "#                 for kernel_size in kernel_sizes:\n",
    "#                     for LSTM_num in LSTM_nums:\n",
    "#                         print(\"--------------开始\"+position+\"_\"+str(window_size)+\"s_卷积层数为\"+str(conv_layer_num)+\"_lstm单元数为\"+str(LSTM_num)+\"_卷积核数为\"+str(filters_num)+\"_卷积核大小为\"+str(kernel_size)+\"的personalized实验--------------\")\n",
    "#                         data=load_data(window_size,position)\n",
    "#                         class_num=5\n",
    "# #                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\"]\n",
    "# #                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\",\"Carpenter1\",\"Carpenter2\",\"Masonry1\",\"Masonry2\"]\n",
    "#                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\"]\n",
    "#                         LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\"]\n",
    "#                 #         for subject in subjects:\n",
    "#                 #             if subject==\"Carpenter1\" or subject==\"Carpenter2\":\n",
    "#                 #                 class_num=5\n",
    "#                 #                 LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\"] \n",
    "#                 # #             elif subject==\"Masonry1\" or subject==\"Masonry2\":\n",
    "#                 # #                 class_num=6\n",
    "#                 # #                 LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\",\"WO\"] \n",
    "#                 #             else:\n",
    "#                 #                 LABELS = [ \"ST\", \"WK\", \"TR\", \"BD\", \"SQ\"]\n",
    "\n",
    "#                 #         print(\"--------------开始第\"+subject+\"--------------\")\n",
    "#                         X,Y,Group=dataset_generate(window_size,data,subjects)\n",
    "#                         distribution_check(Y)\n",
    "#                         Y=int_encoder(Y)\n",
    "#                         # filter=[32,64,128]\n",
    "#                         # kernel_sizes=[3,4,7,9]\n",
    "#                         # LSTM_nums=[16,32,64,128]\n",
    "#                         # for kernel_size in kernel_sizes:\n",
    "#                         average_macrof1_score_total=0.0\n",
    "#                         num=1\n",
    "#                         confusion_matrix_total=np.zeros((class_num,class_num))\n",
    "#                         # print(\"--------------开始LSTM_num为\"+str(kernel_size)+\"的实验--------------\")\n",
    "# #                         for train_total_index, test_index in skf1.split(X, Y):\n",
    "#                         for train_total_index, test_index in logo.split(X, Y,Group):\n",
    "# #                             print(\"--------------开始第\"+str(num)+\"次交叉验证--------------\")\n",
    "#                             print(\"当前为\"+str(Group[test_index[0],]))\n",
    "                            \n",
    "#                             confusion_matrix_subject=np.zeros((class_num,class_num))\n",
    "#                             average_macrof1_score_subject=0.0\n",
    "            \n",
    "#                             X_all, y_all = X[test_index],Y[test_index]\n",
    "#                             print(Counter(y_all))\n",
    "#                             X_train, y_train = [],[]\n",
    "#                             X_validation,y_validation=[],[]\n",
    "#                             number=1\n",
    "#                             for train_index, validation_index in skf1.split(X_all, y_all):\n",
    "#                                 X_train, y_train = X_all[train_index],y_all[train_index]\n",
    "#                                 print(Counter(y_train))\n",
    "#                                 X_train, y_train=smote(X_train, y_train,str(Group[test_index[0],]))\n",
    "#                                 X_validation,y_validation=X_all[validation_index],y_all[validation_index]\n",
    "#                                 print(\"训练集第\"+str(number)+\"次\")\n",
    "#                                 print(\"训练集比例\")\n",
    "#                                 print(Counter(y_train))\n",
    "#                                 print(\"测试集比例\")\n",
    "#                                 print(Counter(y_validation))\n",
    "#                                 model=construct_model(conv_layer_num=conv_layer_num ,kernel_size=kernel_size,filters_num=filters_num,pool=3,window_size_for_model=int(window_size*50),LSTM_num=LSTM_num,class_num=class_num)\n",
    "#                                 model_fit(model,X_train=X_train,y_train=y_train,X_validation=X_validation,y_validation=y_validation,batch_size=64)\n",
    "#                                 results,confusion_matrix_temp=model_test(model,X_test=X_validation,y_test=y_validation,class_num=class_num,LABELS=LABELS)\n",
    "#                                 average_macrof1_score_subject=average_macrof1_score_subject + results\n",
    "#                                 confusion_matrix_subject=confusion_matrix_subject+confusion_matrix_temp\n",
    "#                                 del model\n",
    "#                                 number=number+1\n",
    "#                             print(confusion_matrix_subject)\n",
    "#                             print(str(Group[test_index[0],])+\"_\"+position+\"_\"+str(window_size)+\"的average-macro-f1-score:\"+str(average_macrof1_score_subject/(number-1))+\"%\")\n",
    "#                             #         break\n",
    "#                             num=num+1\n",
    "# #                             break\n",
    "\n",
    "#                 #             plt.figure(figsize=(5,5))\n",
    "#                 #             plt.grid(b=False)\n",
    "#                 #             plot_confusion_matrix(confusion_matrix, classes=LABELS, normalize=True, title='Normalized confusion matrix',cmap=plt.cm.Reds)\n",
    "#                 #             ax = plt.gca()\n",
    "#                 #             plt.show()\n",
    "#                 #             plt.figure(figsize=(5,5))\n",
    "#                 #             plot_confusion_matrix(confusion_matrix, classes=LABELS, normalize=False, title='confusion matrix',cmap=plt.cm.Reds)\n",
    "#                 #             ax = plt.gca()\n",
    "#                 #             plt.show()\n",
    "#                         print(confusion_matrix_total)\n",
    "#                         print(position+\"_\"+str(window_size)+\"的average-macro-f1-score:\"+str(average_macrof1_score_total/(num-1))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SRS划分数据集(generalized model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf1 = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "skf2 = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "logo = LeaveOneGroupOut()\n",
    "# logo.get_n_splits(X, Y, group)\n",
    "\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "lpgo = LeavePGroupsOut(n_groups=2)\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "sgkf=StratifiedGroupKFold(n_splits=3,shuffle=True)\n",
    "\n",
    "window_sizes=[0.8]\n",
    "positions=[\"kd\",\"xd\"]\n",
    "conv_layers_num=[2] #最优2\n",
    "filters_nums=[256] #最优256\n",
    "kernel_sizes=[3]#最优3(4,1)\n",
    "LSTM_nums=[64] #最优64\n",
    "\n",
    "\n",
    "for position in positions:\n",
    "    for window_size in window_sizes:\n",
    "        for conv_layer_num in conv_layers_num:\n",
    "            for filters_num in filters_nums:\n",
    "                for kernel_size in kernel_sizes:\n",
    "                    for LSTM_num in LSTM_nums:\n",
    "                        print(\"--------------开始\"+position+\"_\"+str(window_size)+\"s_卷积层数为\"+str(conv_layer_num)+\"_lstm单元数为\"+str(LSTM_num)+\"_卷积核数为\"+str(filters_num)+\"_卷积核大小为\"+str(kernel_size)+\"的generalized实验--------------\")\n",
    "                        data=load_data(window_size,position)\n",
    "                        class_num=5\n",
    "#                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\"]\n",
    "                        subject_names=[\"Rebar1\",\"Rebar2\",\"Rebar3\",\"Carpenter1\",\"Carpenter2\",\"Masonry1\",\"Masonry2\"]\n",
    "#                         subject_names=[\"David\",\"WGH\",\"FL\",\"RQB\",\"WHT\",\"FX\",\"YJJ\",\"GRH\"]\n",
    "#                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\"]\n",
    "                        LABELS = [ \"1\", \"2\",  \"3\", \"4\", \"5\" ]\n",
    "                #         for subject in subjects:\n",
    "                #             if subject==\"Carpenter1\" or subject==\"Carpenter2\":\n",
    "                #                 class_num=5\n",
    "                #                 LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\"] \n",
    "                # #             elif subject==\"Masonry1\" or subject==\"Masonry2\":\n",
    "                # #                 class_num=6\n",
    "                # #                 LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\",\"WO\"] \n",
    "                #             else:\n",
    "                #                 LABELS = [ \"ST\", \"WK\", \"TR\", \"BD\", \"SQ\"]\n",
    "\n",
    "                #         print(\"--------------开始第\"+subject+\"--------------\")\n",
    "                        X,Y,Group=dataset_generate(window_size,data,subject_names)\n",
    "                        distribution_check(Y)\n",
    "                        Y=int_encoder(Y)\n",
    "                        # filter=[32,64,128]\n",
    "                        # kernel_sizes=[3,4,7,9]\n",
    "                        # LSTM_nums=[16,32,64,128]\n",
    "                        # for kernel_size in kernel_sizes:\n",
    "                        average_macrof1_score=0.0\n",
    "                        average_accuracy=0.0\n",
    "                        num=1\n",
    "                        confusion_matrix=np.zeros((class_num,class_num))\n",
    "                        # print(\"--------------开始LSTM_num为\"+str(kernel_size)+\"的实验--------------\")\n",
    "                        for train_total_index, test_index in skf1.split(X, Y):\n",
    "#                         for train_total_index, test_index in logo.split(X, Y,Group):\n",
    "#                             print(\"--------------开始第\"+str(num)+\"次交叉验证--------------\")\n",
    "#                             print(\"当前测试集为\"+str(Group[test_index[0],]))\n",
    "#                             if str(Group[test_index[0],]) != \"Masonry1\"  : \n",
    "#                                 continue\n",
    "                        #     print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "                        #     X_train, X_test = X[train_index], X[test_index]\n",
    "                        #     y_train, y_test = Y[train_index], Y[test_index]\n",
    "                        #     print(train_total_index)\n",
    "                            X_test, y_test = X[test_index],Y[test_index]\n",
    "                            X_train_total,y_train_total=X[train_total_index],Y[train_total_index]\n",
    "                            print(Counter(y_train_total))\n",
    "                            X_train_total,y_train_total=smote_generalized(X_train_total,y_train_total,window_size)\n",
    "                            print(Counter(y_train_total))\n",
    "                            X_train, y_train = [],[]\n",
    "                            X_validation,y_validation=[],[]\n",
    "                            print(y_train_total)\n",
    "                            number=1\n",
    "                            for train_index, validation_index in skf1.split(X_train_total, y_train_total):\n",
    "                                if number==1:\n",
    "                                    X_train, y_train = X_train_total[train_index],y_train_total[train_index]\n",
    "                                    X_validation,y_validation=X_train_total[validation_index],y_train_total[validation_index]\n",
    "                                    number=number+1\n",
    "                                else:\n",
    "                                    break\n",
    "                        #     Group_train_total=Group[train_total_index]\n",
    "#                             print(train_total_index.shape)\n",
    "#                             print(test_index.shape)\n",
    "#                             print(Counter(y_test))\n",
    "#                             print(Counter(y_train)) \n",
    "#                             print(Counter(y_validation))\n",
    "                        #     print(Counter(y_train_total))\n",
    "                        #     for train_index,validation_index in skf1.split(X_train_total,y_train_total):\n",
    "                        # #         print(\"当前验证集为\"+str(Group_train_total[validation_index[0],]))\n",
    "                        #         X_train, X_validation = X_train_total[train_index], X_train_total[validation_index]\n",
    "                        #         y_train, y_validation = y_train_total[train_index], y_train_total[validation_index]\n",
    "                        #         print(train_index.shape)\n",
    "                        #         print(validation_index.shape)\n",
    "\n",
    "                        #         print(Counter(y_train))\n",
    "                        #         print(Counter(y_validation))\n",
    "        #                     smote(X_train_total,y_train_total)\n",
    "        \n",
    "        \n",
    "#                             X_train=tf.expand_dims(X_train,-1)\n",
    "#                             X_validation=tf.expand_dims(X_validation,-1)\n",
    "#                             X_test=tf.expand_dims(X_test,-1)\n",
    "                            print(X_train.shape,X_validation.shape,X_test.shape)\n",
    "                    \n",
    "                    \n",
    "#                             print(\"输入维度\")\n",
    "#                             print(X_train.shape)\n",
    "                            model=construct_model(conv_layer_num=conv_layer_num ,kernel_size=kernel_size,filters_num=filters_num,pool=3,window_size_for_model=int(window_size*50),LSTM_num=LSTM_num,class_num=class_num,learn_rate=0.00005)\n",
    "                            model_fit(model,X_train=X_train,y_train=y_train,X_validation=X_validation,y_validation= y_validation,batch_size=256,epochs=300)\n",
    "                            results,confusion_matrix_temp=model_test(model,X_test=X_test,y_test=y_test,class_num=class_num,LABELS=LABELS)\n",
    "                        #     print(results['f1-score'])\n",
    "                            average_macrof1_score=average_macrof1_score + results\n",
    "                            confusion_matrix=confusion_matrix+confusion_matrix_temp\n",
    "                            del model\n",
    "                        #         break\n",
    "                            num=num+1\n",
    "                            break\n",
    "\n",
    "                #             plt.figure(figsize=(5,5))\n",
    "                #             plt.grid(b=False)\n",
    "                #             plot_confusion_matrix(confusion_matrix, classes=LABELS, normalize=True, title='Normalized confusion matrix',cmap=plt.cm.Reds)\n",
    "                #             ax = plt.gca()\n",
    "                #             plt.show()\n",
    "                #             plt.figure(figsize=(5,5))\n",
    "                #             plot_confusion_matrix(confusion_matrix, classes=LABELS, normalize=False, title='confusion matrix',cmap=plt.cm.Reds)\n",
    "                #             ax = plt.gca()\n",
    "                #             plt.show()\n",
    "                        print(confusion_matrix)\n",
    "                        print(position+\"_\"+str(window_size)+\"的average-macro-f1-score:\"+str(average_macrof1_score/(num-1))+\"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tf29",
   "language": "python",
   "name": "tf29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
