{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Includes\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"Accelerometer_x\",\n",
    "    \"Accelerometer_y\",\n",
    "    \"Accelerometer_z\",\n",
    "    \"Gyroscope_x\",\n",
    "    \"Gyroscope_y\",\n",
    "    \"Gyroscope_z\",\n",
    "    \"Gravity_x\",\n",
    "    \"Gravity_y\",\n",
    "    \"Gravity_z\",\n",
    "    \"Magnetometer_x\",\n",
    "    \"Magnetometer_y\",\n",
    "    \"Magnetometer_z\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(window_size,position):\n",
    "#     window_size=0.8\n",
    "    path=\"D:/dell-PC/CAR/Data/data/UCI HAR Dataset/total/data_for_dl_category by sheets_\"+position+\".xlsx_\" + str(window_size) + \".csv\"\n",
    "    data=pd.read_csv(path,header=0,index_col=None)\n",
    "    subject_names=[\"Rebar1\",\"Rebar2\",\"Rebar3\",\"Carpenter1\",\"Carpenter2\",\"Masonry1\",\"Masonry2\"]\n",
    "    data.loc[data[\"Label_2\"]==4,\"Label_2\"]=3\n",
    "    data.loc[data[\"Label_2\"]==5,\"Label_2\"]=4\n",
    "    data.loc[data[\"Label_2\"]==6,\"Label_2\"]=3\n",
    "    data.loc[data[\"Label_2\"]==7,\"Label_2\"]=3\n",
    "    data.loc[data[\"Label_2\"]==8,\"Label_2\"]=5\n",
    "    data.head(10)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generate(window_size,data,subject):\n",
    "    # data_temp=data.loc[[\"Rebar2\"],:]\n",
    "    # data_temp=data.loc[data[\"Subject\"]==\"Rebar1\" ,:]\n",
    "    window=int(window_size*50)\n",
    "    data_temp=data[data[\"Subject\"].isin(subject)]\n",
    "#     data_temp=data.loc[[\"Rebar2\"],:]\n",
    "#     data_temp=data.loc[data[\"Subject\"]==subject ,:]\n",
    "    data_subject=np.array(data_temp[data_temp[\"Label_2\"].isin([1,2,3,4,5])])\n",
    "    print(data_subject.shape)\n",
    "    data_subject=np.reshape(data_subject,(-1,window,15))\n",
    "    print(data_subject.shape)\n",
    "    X=data_subject[:,:,3:]\n",
    "    X=X.astype(\"float64\")\n",
    "    Y=data_subject[:,0,2]\n",
    "    Y=Y.astype(\"int\")\n",
    "    Group=data_subject[:,0,1]\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print(Group.shape)\n",
    "    return X,Y,Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据类别分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def distribution_check(Y):\n",
    "    Y_count=np.array(Y[:])\n",
    "    print(Y.shape)\n",
    "    print(Counter(Y_count))\n",
    "#     print(Counter(Group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整数编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def int_encoder(Y):\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y = label_encoder.fit_transform(Y)\n",
    "    print(Counter(Y))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义smote函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn as ib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "def smote(X_train,y_train,subject):\n",
    "    dim0=X_train.shape[0]\n",
    "    dim1=X_train.shape[1]\n",
    "    dim2=X_train.shape[2]\n",
    "    X_trian_for_smote=np.reshape(X_train,(dim0,dim1*dim2))\n",
    "    \n",
    "#     sme = SMOTETomek(random_state=42)\n",
    "#     X_train_smo, y_train = sme.fit_resample(X_trian_for_smote, y_train)\n",
    "#     rus = RandomUnderSampler(random_state=42,sampling_strategy='majority')\n",
    "#     X_res, y_res = rus.fit_resample(X_trian_for_smote, y_train)\n",
    "    if subject==\"Carpenter1\":\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={1:500,2:500})\n",
    "    elif subject==\"Carpenter2\":\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={2:500})\n",
    "#     elif subject==\"Masonry1\" or \"Masonry2\":\n",
    "#         smo = SMOTE(random_state=42,sampling_strategy={1:300})\n",
    "    elif subject==\"Rebar1\" or \"Rebar2\" or \"Rebar3\":\n",
    "        smo = SMOTE(random_state=42,sampling_strategy={2:300,3:300})\n",
    "\n",
    "    X_train_smo, y_train = smo.fit_resample(X_trian_for_smote, y_train)\n",
    "#     cnn = CondensedNearestNeighbour(random_state=42,sampling_strategy='majority') \n",
    "#     X_res, y_train = cnn.fit_resample(X_trian_for_smote, y_train ) \n",
    "    X_train=np.reshape(X_train_smo,(X_train_smo.shape[0],dim1,dim2))\n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "# # from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM\n",
    "# def construct_model(kernel_size,filters_num,pool,window_size_for_model,LSTM_num,class_num):\n",
    "#     model = Sequential([\n",
    "#         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same',input_shape=(window_size_for_model,12), name = 'cnn_1'),\n",
    "#         BatchNormalization(name='batchnorm_layer_1'),\n",
    "# #         MaxPooling1D(pool_size=pool, name = 'maxpooling_1'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_2'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_2'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_3'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_3'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_4'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_5'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_4'),\n",
    "# #         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# # #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_5'),\n",
    "# #         Flatten(),\n",
    "#         LSTM(LSTM_num,return_sequences = True, name = 'lstm_1'),\n",
    "#         Dropout(0.4, name = 'Dropout_1'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_1'),\n",
    "#         LSTM(LSTM_num, return_sequences = False, name = 'lstm_2'),\n",
    "#         Dropout(0.4, name = 'Dropout_2'),\n",
    "# #         LSTM(LSTM_num, return_sequences = False, name = 'lstm_3'),\n",
    "# #         Dropout(0.4, name = 'Dropout_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_2'),\n",
    "#         Flatten(),\n",
    "#     #     Dense(128, activation= 'relu', name = 'dense_2'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_4'),\n",
    "#     #     Dense(64, activation = 'relu', name = 'dense_3'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_5'),\n",
    "# #         Dense(32, activation= 'relu', name = 'dense_1'),\n",
    "#     #     Dropout(0.3, name = 'Dropout_3'),\n",
    "#         Dense(class_num, activation = 'softmax', name = 'output')])\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "#     model.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#     # plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM,AveragePooling1D\n",
    "def construct_model(kernel_size,filters_num,pool,window_size_for_model,LSTM_num,class_num,conv_layer_num):\n",
    "    model = Sequential()\n",
    "    for i in range(1,conv_layer_num+1):\n",
    "        if i==1:\n",
    "            model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(AveragePooling1D(pool_size=pool))\n",
    "#             model.add(MaxPooling1D(pool_size=pool))\n",
    "        else:\n",
    "            model.add(Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same',input_shape=(window_size_for_model,12)))\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "#         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_2'),\n",
    "# #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "#         BatchNormalization(name='batchnorm_layer_2'),\n",
    "#         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_3'),\n",
    "# #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "#         BatchNormalization(name='batchnorm_layer_3'),\n",
    "#         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# #         MaxPooling1D(pool_size=pool, name = 'maxpooling_2'),\n",
    "#         BatchNormalization(name='batchnorm_layer_4'),\n",
    "#         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "#         BatchNormalization(name='batchnorm_layer_5'),\n",
    "#         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_4'),\n",
    "# #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "#         BatchNormalization(name='batchnorm_layer_4'),\n",
    "#         Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_5'),\n",
    "# #         MaxPooling1D(pool_size=pool, name = 'maxpooling_3'),\n",
    "#         BatchNormalization(name='batchnorm_layer_5'),\n",
    "#         Flatten(),\n",
    "#     model.add(LSTM(LSTM_num,input_shape=(window_size_for_model,12),return_sequences = True, name = 'lstm_1'))\n",
    "#     model.add(Dropout(0.4, name = 'Dropout_1'))\n",
    "# #         BatchNormalization(name='batchnorm_layer_1'),\n",
    "#     model.add(LSTM(LSTM_num, return_sequences = False, name = 'lstm_2'))\n",
    "#     model.add(Dropout(0.4, name = 'Dropout_2'))\n",
    "#         LSTM(LSTM_num, return_sequences = False, name = 'lstm_3'),\n",
    "#         Dropout(0.4, name = 'Dropout_3'),\n",
    "#         BatchNormalization(name='batchnorm_layer_2'),\n",
    "    model.add(Dense(256, activation = 'sigmoid'))\n",
    "    model.add(Dense(256, activation = 'sigmoid'))\n",
    "    model.add(Flatten())\n",
    "    #     Dense(128, activation= 'relu', name = 'dense_2'),\n",
    "    #     Dropout(0.3, name = 'Dropout_4'),\n",
    "    #     Dense(64, activation = 'relu', name = 'dense_3'),\n",
    "    #     Dropout(0.3, name = 'Dropout_5'),\n",
    "#         Dense(32, activation= 'relu', name = 'dense_1'),\n",
    "    #     Dropout(0.3, name = 'Dropout_3'),\n",
    "    model.add(Dense(class_num, activation = 'softmax', name = 'output'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    # plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "# # from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM\n",
    "# import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.layers import Lambda,Input\n",
    "# def construct_model(kernel_size,filters_num,pool,window_size_for_model,LSTM_num,class_num):\n",
    "# #     model = Sequential()\n",
    "#     input=Input(shape=(window_size_for_model,12))\n",
    "#     x=Conv1D(filters=filters_num, kernel_size=kernel_size, activation='relu',padding = 'same', name = 'cnn_1')(input)\n",
    "#     x=BatchNormalization(name='batchnorm_layer_1')(x)\n",
    "# #         Flatten(),\n",
    "#     x=Flatten()(x)\n",
    "    \n",
    "#     x=Lambda(lambda x:K.expand_dims(x,axis=-1))(x)\n",
    "    \n",
    "#     x=LSTM(LSTM_num,return_sequences = True, name = 'lstm_1')(x)\n",
    "#     x=Dropout(0.4, name = 'Dropout_1')(x)\n",
    "# #         BatchNormalization(name='batchnorm_layer_1'),\n",
    "#     x=LSTM(LSTM_num, return_sequences = False, name = 'lstm_2')(x)\n",
    "#     x=Dropout(0.4, name = 'Dropout_2')(x)\n",
    "# #         LSTM(LSTM_num, return_sequences = False, name = 'lstm_3'),\n",
    "# #         Dropout(0.4, name = 'Dropout_3'),\n",
    "# #         BatchNormalization(name='batchnorm_layer_2'),\n",
    "# #     Dense(128, activation= 'relu', name = 'dense_2'),\n",
    "# #     Dropout(0.3, name = 'Dropout_4'),\n",
    "# #     Dense(64, activation = 'relu', name = 'dense_3'),\n",
    "# #     Dropout(0.3, name = 'Dropout_5'),\n",
    "#     x=Dense(32, activation= 'relu', name = 'dense_1')(x)\n",
    "# #     Dropout(0.3, name = 'Dropout_3'),\n",
    "#     predic=Dense(class_num, activation = 'softmax', name = 'output')(x)\n",
    "    \n",
    "#     model=Model(inputs=input,outputs=predic)\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "#     model.compile(optimizer=opt, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "#     # plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from timeit import default_timer as timer\n",
    "def model_fit(model,X_train, y_train,X_validation,y_validation,batch_size):\n",
    "    earlystop_callback=EarlyStopping(monitor=\"val_loss\",patience=5)\n",
    "    modelcheckpoint=ModelCheckpoint(filepath=\"best_model.h5\",monitor=\"val_accuracy\",save_best_only=True)\n",
    "    start = timer()\n",
    "#     class_weight = {0 : 1., 1: 10., 2: 10., 3: 5., 4: 3.}\n",
    "\n",
    "    history = model.fit( X_train, y_train, epochs = 300,validation_data = (X_validation,y_validation), batch_size=batch_size ,callbacks=[earlystop_callback,modelcheckpoint], verbose=1)\n",
    "#     validation_data = (X_validation,y_validation)\n",
    "\n",
    "    end = timer()\n",
    "    print(\"\\n\")\n",
    "    print(\"Time: \",(end - start),\"secs = \",(end - start)/3600,\"hours\")\n",
    "\n",
    "\n",
    "    # ## EVALUATION AND THE PERFORMANCE METRICS\n",
    "\n",
    "    # In[62]:\n",
    "\n",
    "\n",
    "    # Plotting loss and accuracy graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.plot(np.array(history.history['loss']), \"r--\", label=\"Train loss\")\n",
    "    plt.plot(np.array(history.history['accuracy']), \"g--\", label=\"Train accuracy\")\n",
    "\n",
    "    plt.plot(np.array(history.history['val_loss']), \"r-\", label=\"validation loss\")\n",
    "    plt.plot(np.array(history.history['val_accuracy']), \"g-\", label=\"validation accuracy\")\n",
    "\n",
    "    plt.title(\"Training session's progress over iterations(imbalanced data)\")\n",
    "    plt.legend(loc='upper right', shadow=True)\n",
    "    plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylim(0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
    "def plot_confusion_matrix(cm, classes,class_num,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0,fontsize=25)\n",
    "    plt.yticks(tick_marks, classes,fontsize=25)\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(-.5, class_num-0.5)\n",
    "    ax.set_xlim(-.5, class_num-0.5)\n",
    "        \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",fontsize=20,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=20)\n",
    "    plt.xlabel('Predicted label',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型预测以及结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model,X_test,y_test,class_num,LABELS):\n",
    "#     results = dict()\n",
    "    test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "#     print(test_pred)\n",
    "    #输出分类结果\n",
    "#     print(y_test)\n",
    "#     print(test_pred)\n",
    "    classfication_report=metrics.classification_report(y_test,test_pred)\n",
    "    results=float(100*metrics.f1_score(y_test, test_pred, average=\"macro\"))\n",
    "#     results['classification_report'] = classification_report\n",
    "    print(classfication_report)\n",
    "    #输出混淆矩阵\n",
    "    cm = metrics.confusion_matrix(y_test, test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    #绘制归一化混淆矩阵\n",
    "    plt.figure(figsize=(class_num,class_num))\n",
    "    plt.grid(b=False)\n",
    "    plot_confusion_matrix(cm, LABELS,class_num, normalize=True, title='Normalized confusion matrix',cmap=plt.cm.Reds)\n",
    "    ax = plt.gca()\n",
    "    plt.show()\n",
    "    #绘制非归一化混淆矩阵\n",
    "    plt.figure(figsize=(class_num,class_num))\n",
    "    plot_confusion_matrix(cm, LABELS,class_num, normalize=False, title='confusion matrix',cmap=plt.cm.Reds)\n",
    "    ax = plt.gca()\n",
    "    plt.show()\n",
    "    return results,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf1 = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "# skf2 = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# logo = LeaveOneGroupOut()\n",
    "# # logo.get_n_splits(X, Y, group)\n",
    "\n",
    "# from sklearn.model_selection import LeavePGroupsOut\n",
    "# lpgo = LeavePGroupsOut(n_groups=2)\n",
    "\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# sgkf=StratifiedGroupKFold(n_splits=3,shuffle=True)\n",
    "\n",
    "# window_sizes=[0.6]\n",
    "# positions=[\"xd\"]\n",
    "# conv_layers_num=[2] #最优2\n",
    "# filters_nums=[256] #最优256\n",
    "# kernel_sizes=[3] #最优3\n",
    "# LSTM_nums=[64] #最优64\n",
    "\n",
    "# for position in positions:\n",
    "#     for window_size in window_sizes:\n",
    "#         for conv_layer_num in conv_layers_num:\n",
    "#             for filters_num in filters_nums:\n",
    "#                 for kernel_size in kernel_sizes:\n",
    "#                     for LSTM_num in LSTM_nums:\n",
    "#                         print(\"--------------开始\"+position+\"_\"+str(window_size)+\"s_卷积层数为\"+str(conv_layer_num)+\"_lstm单元数为\"+str(LSTM_num)+\"_卷积核数为\"+str(filters_num)+\"_卷积核大小为\"+str(kernel_size)+\"的personalized实验--------------\")\n",
    "#                         data=load_data(window_size,position)\n",
    "#                         class_num=5\n",
    "# #                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\"]\n",
    "# #                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\",\"Carpenter1\",\"Carpenter2\",\"Masonry1\",\"Masonry2\"]\n",
    "#                         subjects=[\"Rebar1\",\"Rebar2\",\"Rebar3\"]\n",
    "#                         LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\"]\n",
    "#                 #         for subject in subjects:\n",
    "#                 #             if subject==\"Carpenter1\" or subject==\"Carpenter2\":\n",
    "#                 #                 class_num=5\n",
    "#                 #                 LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\"] \n",
    "#                 # #             elif subject==\"Masonry1\" or subject==\"Masonry2\":\n",
    "#                 # #                 class_num=6\n",
    "#                 # #                 LABELS = [ \"ST\", \"WK\",  \"TR\", \"BD\", \"SQ\",\"WO\"] \n",
    "#                 #             else:\n",
    "#                 #                 LABELS = [ \"ST\", \"WK\", \"TR\", \"BD\", \"SQ\"]\n",
    "\n",
    "#                 #         print(\"--------------开始第\"+subject+\"--------------\")\n",
    "#                         X,Y,Group=dataset_generate(window_size,data,subjects)\n",
    "#                         distribution_check(Y)\n",
    "#                         Y=int_encoder(Y)\n",
    "#                         # filter=[32,64,128]\n",
    "#                         # kernel_sizes=[3,4,7,9]\n",
    "#                         # LSTM_nums=[16,32,64,128]\n",
    "#                         # for kernel_size in kernel_sizes:\n",
    "#                         average_macrof1_score_total=0.0\n",
    "#                         num=1\n",
    "#                         confusion_matrix_total=np.zeros((class_num,class_num))\n",
    "#                         # print(\"--------------开始LSTM_num为\"+str(kernel_size)+\"的实验--------------\")\n",
    "# #                         for train_total_index, test_index in skf1.split(X, Y):\n",
    "#                         for train_total_index, test_index in logo.split(X, Y,Group):\n",
    "# #                             print(\"--------------开始第\"+str(num)+\"次交叉验证--------------\")\n",
    "#                             print(\"当前为\"+str(Group[test_index[0],]))\n",
    "                            \n",
    "#                             confusion_matrix_subject=np.zeros((class_num,class_num))\n",
    "#                             average_macrof1_score_subject=0.0\n",
    "            \n",
    "#                             X_all, y_all = X[test_index],Y[test_index]\n",
    "#                             print(Counter(y_all))\n",
    "#                             X_train, y_train = [],[]\n",
    "#                             X_validation,y_validation=[],[]\n",
    "#                             number=1\n",
    "#                             for train_index, validation_index in skf1.split(X_all, y_all):\n",
    "#                                 X_train, y_train = X_all[train_index],y_all[train_index]\n",
    "#                                 print(Counter(y_train))\n",
    "#                                 X_train, y_train=smote(X_train, y_train,str(Group[test_index[0],]))\n",
    "#                                 X_validation,y_validation=X_all[validation_index],y_all[validation_index]\n",
    "#                                 print(\"训练集第\"+str(number)+\"次\")\n",
    "#                                 print(\"训练集比例\")\n",
    "#                                 print(Counter(y_train))\n",
    "#                                 print(\"测试集比例\")\n",
    "#                                 print(Counter(y_validation))\n",
    "#                                 model=construct_model(conv_layer_num=conv_layer_num ,kernel_size=kernel_size,filters_num=filters_num,pool=3,window_size_for_model=int(window_size*50),LSTM_num=LSTM_num,class_num=class_num)\n",
    "#                                 model_fit(model,X_train=X_train,y_train=y_train,X_validation=X_validation,y_validation=y_validation,batch_size=64)\n",
    "#                                 results,confusion_matrix_temp=model_test(model,X_test=X_validation,y_test=y_validation,class_num=class_num,LABELS=LABELS)\n",
    "#                                 average_macrof1_score_subject=average_macrof1_score_subject + results\n",
    "#                                 confusion_matrix_subject=confusion_matrix_subject+confusion_matrix_temp\n",
    "#                                 del model\n",
    "#                                 number=number+1\n",
    "#                             print(confusion_matrix_subject)\n",
    "#                             print(str(Group[test_index[0],])+\"_\"+position+\"_\"+str(window_size)+\"的average-macro-f1-score:\"+str(average_macrof1_score_subject/(number-1))+\"%\")\n",
    "#                             #         break\n",
    "#                             num=num+1\n",
    "# #                             break\n",
    "\n",
    "#                 #             plt.figure(figsize=(5,5))\n",
    "#                 #             plt.grid(b=False)\n",
    "#                 #             plot_confusion_matrix(confusion_matrix, classes=LABELS, normalize=True, title='Normalized confusion matrix',cmap=plt.cm.Reds)\n",
    "#                 #             ax = plt.gca()\n",
    "#                 #             plt.show()\n",
    "#                 #             plt.figure(figsize=(5,5))\n",
    "#                 #             plot_confusion_matrix(confusion_matrix, classes=LABELS, normalize=False, title='confusion matrix',cmap=plt.cm.Reds)\n",
    "#                 #             ax = plt.gca()\n",
    "#                 #             plt.show()\n",
    "#                         print(confusion_matrix_total)\n",
    "#                         print(position+\"_\"+str(window_size)+\"的average-macro-f1-score:\"+str(average_macrof1_score_total/(num-1))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SRS划分数据集(generalized model)"
   ]
  }
